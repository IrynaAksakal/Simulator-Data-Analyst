{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517bc8d3",
   "metadata": {},
   "source": [
    "### 8. Поиск аномалий (система алертов)\n",
    "##### Задача: напишите систему алертов для нашего приложения\n",
    "Система должна с периодичностью каждые 15 минут проверять ключевые метрики, такие как активные пользователи в ленте / мессенджере, просмотры, лайки, CTR, количество отправленных сообщений. \n",
    "1. Изучите поведение метрик и подберите наиболее подходящий метод для детектирования аномалий. \n",
    "2. В случае обнаружения аномального значения, в чат должен отправиться алерт - сообщение со следующей информацией: метрика, ее значение, величина отклонения. В сообщение можно добавить дополнительную информацию, которая поможет при исследовании причин возникновения аномалии, это может быть, например,  график, ссылки на дашборд/чарт в BI системе. \n",
    "3. Автоматизируйте систему алертов с помощью Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ddccb5",
   "metadata": {},
   "source": [
    "Изучив на дашборде поведение метрик за последние 30 дней и за день, пришла к следущим выводам. <br>\n",
    "Не считая некоторых точечных аномалий, которые были в рассматриваемый период. <br>\n",
    "1. в целом с 1 ноября колебания метрик в большую или меньшую сторону почти не заметно: DAU_feed, likes, sent_message\n",
    "2. DAU_message - есть закономерность в поведении метрики: 1 день значение метрики почти в 2 раза выше, чем значение в 2 следующих дня\n",
    "3. views - закономерность в поведении метрики выявить не смогла, в разные дни она ведет себя по-разному. Но заметны в целом в течение недели колебания вверх-вниз. \n",
    "4. Колебания значений метрик в разное время суток в течение дня наблюдается у каждой метрики\n",
    "##### Поэтому для DAU_message для сравнения беру метрики, которое были 3 дня назад, а для остальных метрик - средние значения за 7 последних дней и проверяю ключевые метрики с периодичностью каждые 15 минут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac76eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # для нахождения даты \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import telegram\n",
    "import pandahouse as ph\n",
    "from datetime import date, datetime, timedelta\n",
    "import io # сохраняю картинки в буфере\n",
    "\n",
    "#скрыть токен и chat_id, когда выложу в гите\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "#from airflow.operators.python import get_current_contex\n",
    "\n",
    "\n",
    "#для подключения к clickhouse\n",
    "connection = {\n",
    "              'host':'https://clickhouse.lab.karpov.courses',\n",
    "              'database':'simulator_20221020',\n",
    "              'user':'student', \n",
    "              'password':'dpo_python_2020'\n",
    "             }\n",
    "\n",
    "\n",
    "# Дефолтные параметры, которые прокидываются в таски\n",
    "default_args = {\n",
    "                'owner': 'aksakal',\n",
    "                'depends_on_past': False,\n",
    "                'retries': 2,\n",
    "                'retry_delay': timedelta(minutes=5),\n",
    "                'start_date': datetime(2022, 11, 9), # старт дата, с которой собираем данные9/11/2022\n",
    "                }\n",
    "\n",
    "# Интервал запуска DAG\n",
    "schedule_interval = '*/15 * * * *'# каждые 15 минут запускаю проверку\n",
    "\n",
    "\n",
    "def check_anomaly(df, metric, a=5, n=3):\n",
    "#a- это коэффициент, который будет влиять на ширину межквартильного интервала - смотрим на графике, правильно ли подобралa,\n",
    "#чтобы не было часто аномалий или наоборот, чтобы не получился слишком широкий интервал\n",
    "#n- коэффициент, который будет влиять на количество временных промежутков, для которых рассчитываю межквартильный интервал \n",
    "# т.е. беру несколько промежутков по 15 минут, которые имеют похожие значения метрик, для которых буду применять \n",
    "# межквартильный интервал.\n",
    "    today = np.datetime64('today')\n",
    "\n",
    "\n",
    "    df['q25'] = df[metric].shift(1).rolling(n).quantile(0.25)\n",
    "    df['q75'] = df[metric].shift(1).rolling(n).quantile(0.75)\n",
    "\n",
    "    # метод shift() - использую для того, чтобы на значения границ, которые буду вычислять для 15 минутки, не повлияли значения \n",
    "    # самой этой 15-минутки, для этого сдвигаю наше окно на 1 период назад\n",
    "    # рассчитываю скользящее значения межквартильного размаха для метрики- для этого сначала рассчитываю 25 и 75 квантили \n",
    "    # отображать буду все значения за вчера и сегодня, чтобы график был наглядным и мб понять, как метрика вела себя в течение дня \n",
    "\n",
    "\n",
    "    # рассчитываю межквартильный размах и записываю в датафрейм\n",
    "    df['iqr'] = df['q75'] - df['q25']  # межквартильный размах\n",
    "    df['up'] = df['q75'] + a * df['iqr'] # верхняя граница межквартильного размаха\n",
    "    df['low'] = df['q25'] - a * df['iqr'] # нижняя граница межквартильного размаха\n",
    "\n",
    "    #чтобы границы не были рваными, еще раз сгладила границы - рассчитала границы скользящим средним\n",
    "    df['up'] = df['up'].rolling(n, center=True, min_periods=1).mean() \n",
    "    df['low'] = df['low'].rolling(n, center=True, min_periods=1).mean()\n",
    "\n",
    "    # center=True - значит текущая 15-минутка будет посередине этого окна\n",
    "    # min_period=1 - задаем, чтобы границы на графике прорисовывались до конца, иначе, поскольку выбрала окно центральным \n",
    "    # (center=True) + если не хватает периодов n для последней 15-минутки, то границы не просчитываются, а проставляются None \n",
    "    # и на графике границы до конца не прорисовываются\n",
    "\n",
    "\n",
    "    # прописываю условие чему равен флаг is_alert\n",
    "    if df[metric].iloc[-1] < df['low'].iloc[-1] or df[metric].iloc[-1] > df['up'].iloc[-1]:\n",
    "        is_alert=1\n",
    "    else:\n",
    "        is_alert=0\n",
    "\n",
    "    df = df[df['date'] == today]\n",
    "\n",
    "    return is_alert, df\n",
    "    # функция будет возвращать: \n",
    "    # 1. флаг is_alert, который говорит о том, являеется ли текущее значение метрики аномальным или нет\n",
    "    # 2. df -в который добавила границы межквартильного размаха\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dag(default_args=default_args, schedule_interval=schedule_interval, catchup=False)\n",
    "def aksakal_my_alert():\n",
    "\n",
    "    today = np.datetime64('today')\n",
    "\n",
    "\n",
    "    # считаю средние показатели метрик за 7 дней до сегодня + ставлю дату за вчера\n",
    "    @task\n",
    "    def extract_feed_7day():\n",
    "        query = ''' SELECT \n",
    "                            hm \n",
    "                            , avg(users_feed) as users_feed\n",
    "                            , avg(views) as views\n",
    "                            , avg(likes) as likes\n",
    "                            , avg(ctr) as ctr\n",
    "                            , date\n",
    "                    FROM\n",
    "                        (SELECT\n",
    "                            formatDateTime(toStartOfFifteenMinutes(time), '%R') as hm\n",
    "                            , uniqExact(user_id) as users_feed\n",
    "                            , countIf(user_id, action = 'view') as views\n",
    "                            , countIf(user_id, action = 'like') as likes\n",
    "                            , likes/views as ctr\n",
    "                            , today()-1 as date\n",
    "                        FROM \n",
    "                            {db}.feed_actions\n",
    "                        WHERE \n",
    "                            time >=  today() - 7 and time < today()\n",
    "\n",
    "                        GROUP BY  date, hm\n",
    "                        ORDER BY date, hm)\n",
    "\n",
    "                    GROUP BY  date, hm\n",
    "                    ORDER BY date, hm\n",
    "                    '''\n",
    "        df_cube_feed_7day = ph.read_clickhouse(query, connection=connection)\n",
    "        return df_cube_feed_7day\n",
    "\n",
    "\n",
    "    # считаю метрики только за сегодня\n",
    "    @task\n",
    "    def extract_feed_today():\n",
    "        query = ''' SELECT\n",
    "                        toStartOfFifteenMinutes(time) as ts\n",
    "                        , toDate(time) as date\n",
    "                        , formatDateTime(ts, '%R') as hm\n",
    "                        , uniqExact(user_id) as users_feed\n",
    "                        , countIf(user_id, action = 'view') as views\n",
    "                        , countIf(user_id, action = 'like') as likes\n",
    "                        , likes/views as ctr\n",
    "\n",
    "                    FROM \n",
    "                        {db}.feed_actions\n",
    "                    WHERE \n",
    "                        time >= today() and time < toStartOfFifteenMinutes(now())\n",
    "\n",
    "                    GROUP BY date, hm, ts\n",
    "                    ORDER BY date, hm \n",
    "                    '''\n",
    "        df_cube_feed_today = ph.read_clickhouse(query, connection=connection)\n",
    "        return df_cube_feed_today\n",
    "\n",
    "\n",
    "    # WHERE time >=  today() and time < toStartOfFifteenMinutes(now()) - беру период за вчера до сегодня без текущих 15 минут,\n",
    "    # тк текущая 15-минутка может быть неполная и тогда получу алерт, что данные упали, хотя это не так\n",
    "    # toStartOfFifteenMinutes(now()) - округляет текущее время до начала текущей 15 минутки \n",
    "    # formatDateTime(time, '%R') as hm  - округленное до 15 минуток время выводит в формате часы-минуты \n",
    "    # добавила колонки date, hm - для удобства построения графиков \n",
    "\n",
    "\n",
    "\n",
    "    # считаю метрики за 3 дня до сегодня + сегодня для users_message, тк там есть закономерность каждые 3 дня (1 день рост, 2 снижение и так циклично)\n",
    "    @task\n",
    "    def extract_users_message():\n",
    "        query = ''' SELECT \n",
    "                            ts, date, hm, users_message\n",
    "                    FROM\n",
    "\n",
    "                        (SELECT\n",
    "                            toStartOfFifteenMinutes(time) as ts\n",
    "                            , toDate(time) as date\n",
    "                            , formatDateTime(ts, '%R') as hm\n",
    "                            , uniqExact(user_id) as users_message\n",
    "\n",
    "                        FROM \n",
    "                        {db}.message_actions\n",
    "                        WHERE \n",
    "                        time >= today() and time < toStartOfFifteenMinutes(now())\n",
    "\n",
    "                        GROUP BY date, hm, ts\n",
    "                        ORDER BY date)\n",
    "\n",
    "                        UNION ALL               \n",
    "\n",
    "                    (SELECT\n",
    "                            toStartOfFifteenMinutes(time) as ts\n",
    "                            , toDate(time) as date\n",
    "                            , formatDateTime(ts, '%R') as hm\n",
    "                            , uniqExact(user_id) as users_message\n",
    "\n",
    "                        FROM \n",
    "                        {db}.message_actions\n",
    "                        WHERE \n",
    "                            time >= today()-3 and time < today()-2\n",
    "\n",
    "                        GROUP BY date, hm, ts\n",
    "                        ORDER BY date)\n",
    "                    '''\n",
    "        df_cube_users_message = ph.read_clickhouse(query, connection=connection)\n",
    "        return df_cube_users_message\n",
    "\n",
    "\n",
    "    # считаю средние показатели метрик за 7 дней до сегодня + ставлю дату за вчера\n",
    "    # буду смотреть аномалии по алгоритму, как для метрик feed \n",
    "    @task\n",
    "    def extract_sent_message():\n",
    "        query = ''' \n",
    "                SELECT \n",
    "                            hm\n",
    "                            , date\n",
    "                            , avg(sent_message) as sent_message\n",
    "                FROM\n",
    "\n",
    "                    (SELECT\n",
    "                        today()-1 as date\n",
    "                        , formatDateTime(toStartOfFifteenMinutes(time), '%R') as hm\n",
    "                        , count(user_id) as sent_message\n",
    "\n",
    "                    FROM \n",
    "                    {db}.message_actions\n",
    "                    WHERE \n",
    "                        time >=  today()-7 and time < today()\n",
    "\n",
    "                    GROUP BY date, hm\n",
    "                    ORDER BY date, hm)  \n",
    "\n",
    "                GROUP BY date, hm\n",
    "                ORDER BY date, hm      \n",
    "                    '''\n",
    "        df_cube_sent_message = ph.read_clickhouse(query, connection=connection)\n",
    "        return df_cube_sent_message\n",
    "\n",
    "\n",
    "    @task\n",
    "    def extract_sent_message_today():\n",
    "        query = ''' \n",
    "                SELECT \n",
    "                        toStartOfFifteenMinutes(time) as ts\n",
    "                        , toDate(time) as date\n",
    "                        , formatDateTime(ts, '%R') as hm\n",
    "                        , count(user_id) as sent_message\n",
    "\n",
    "                    FROM \n",
    "                        {db}.message_actions\n",
    "                    WHERE \n",
    "                        time >= today() and time < toStartOfFifteenMinutes(now())\n",
    "\n",
    "                    GROUP BY date, hm, ts\n",
    "                    ORDER BY date, hm \n",
    "                    '''\n",
    "        df_cube_sent_message_today = ph.read_clickhouse(query, connection=connection)\n",
    "        return df_cube_sent_message_today\n",
    "\n",
    "\n",
    "    #объединяю данные из 2 датафреймов в 1 для выявления аномалий в метриках\n",
    "    @task\n",
    "    def transform_data_feed(df_cube_feed_7day, df_cube_feed_today):\n",
    "\n",
    "        df_cube_feed_7day['ts'] = df_cube_feed_7day['date'].astype ( str )+' '+df_cube_feed_7day['hm']\n",
    "        df_cube_feed_7day['ts'] = pd.to_datetime(df_cube_feed_7day['ts'], format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "        data_feed = pd.concat([df_cube_feed_7day, df_cube_feed_today], ignore_index=True)\n",
    "        data_feed = data_feed.fillna(0)   # заменяю пропущенные значения NaN на 0  \n",
    "        \n",
    "        return data_feed\n",
    "\n",
    "    @task\n",
    "    def create_data_feed_dau(data_feed): \n",
    "        data_feed_dau = data_feed[['hm', 'date', 'ts', 'users_feed']].copy()\n",
    "        return data_feed_dau\n",
    "        \n",
    "    @task\n",
    "    def create_data_feed_views(data_feed): \n",
    "        data_feed_views = data_feed[['hm', 'date', 'ts', 'views']].copy()\n",
    "        return data_feed_views\n",
    "\n",
    "    @task\n",
    "    def create_data_feed_likes(data_feed): \n",
    "        data_feed_likes = data_feed[['hm', 'date', 'ts', 'likes']].copy()\n",
    "        return data_feed_likes\n",
    "\n",
    "    @task\n",
    "    def create_data_feed_ctr(data_feed): \n",
    "        data_feed_ctr =  data_feed[['hm', 'date', 'ts', 'ctr']].copy()\n",
    "        return data_feed_ctr    \n",
    "\n",
    "\n",
    "    #объединяю данные из 2 датафреймов в 1 для выявления аномалий в метриках\n",
    "    @task\n",
    "    def transform_data_sent_message(df_cube_sent_message, df_cube_sent_message_today):\n",
    "\n",
    "        df_cube_sent_message['ts'] = df_cube_sent_message['date'].astype ( str )+' '+ df_cube_sent_message['hm']\n",
    "        df_cube_sent_message['ts'] = pd.to_datetime(df_cube_sent_message['ts'], format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "        data_sent_message = pd.concat([df_cube_sent_message, df_cube_sent_message_today], ignore_index=True)\n",
    "\n",
    "        return data_sent_message\n",
    "\n",
    "\n",
    "    # функция, которая запускает систему алертов и проверяем метрику на аномальность алгоритмом, описаным в функции check_anomaly()\n",
    "    @task\n",
    "    def run_alerts(df_1, metrics):\n",
    "        # данные для подключения к боту\n",
    "        my_token = 'token' # заменила настоящий токен, поскольку репозиторий публичный\n",
    "        bot = telegram.Bot(token=my_token) # получаю доступ к бот\n",
    "        chat_id = id_number # заменила настоящий id, поскольку репозиторий публичный\n",
    "\n",
    "        df = df_1\n",
    "        metric = metrics\n",
    "\n",
    "        #для каждой метрики вызываю функцию поиска аномалий и возвращаю df и флаг, является ли текущее значениe 15минутки аномалией   \n",
    "        is_alert, df = check_anomaly(df, metric) \n",
    "\n",
    "    # функция, которая отправляет текст и график, если текущее значение - аномалия\n",
    "        if is_alert==1: \n",
    "            #!!!!!пока проверяю работу аллерта поставила, что не аллерт ==0: нужно потом поменять\n",
    "            msg = '''Метрика {metric}:\\n- текущее значение {current_val: .2f} \\n- отклонение от предыдущего значения {last_val_diff: .2%}\\nhttps://superset.lab.karpov.courses/superset/dashboard/2208/'''\\\n",
    "            .format(metric = metric, current_val=df[metric].iloc[-1], \\\n",
    "                    last_val_diff=abs(1-(df[metric].iloc[-1]/df[metric].iloc[-2]))) \n",
    "\n",
    "    # ссылка на дашборд с метриками за день в разрезе 15 минут, чтобы искать причину аномалий \n",
    "    # https://superset.lab.karpov.courses/superset/dashboard/2208/\n",
    "    # {current_val: .2f} - округлила до 2 знаков\n",
    "    # {last_val_diff: .2%} - отклонение в процентах от предыдущей 15-минутки\n",
    "    # last_val_diff=abs( 1-(df[metric].iloc[-1]/df[metric].iloc[-2]) - вычисляю в %, поэтому abs(1-)\n",
    "    # df[metric].iloc[-1]/df[metric].iloc[-2] - отношение значения текущей 15-минутки df[metric].iloc[-1], \n",
    "    # к значению предыдущей df[metric].iloc[-2] \n",
    "\n",
    "\n",
    "            # задаю параметр - размер изображения - на графике будет весь текущий день\n",
    "            sns.set(rc={'figure.figsize': (16,10)})\n",
    "            plt.tight_layout()\n",
    "\n",
    "\n",
    "            #строю графики с данными за текущий день и прошлый день/среднее за 10 дней !!!!!!!!!!!\n",
    "            ax = sns.lineplot(x= df['ts'], y= df[metric], label='metric')\n",
    "            ax = sns.lineplot(x= df['ts'], y=df['up'], label='up')\n",
    "            ax = sns.lineplot(x= df['ts'], y= df['low'], label='low')\n",
    "\n",
    "\n",
    "            # разрядим подписи по оси х, чтобы был подписан не каждый тик по оси х\n",
    "            # циклом задаем, как обозначать на оси х время, чтобы не каждое значение прописывалось\n",
    "            for ind, label in enumerate(ax.get_xticklabels()):\n",
    "                if ind % 2 == 0:\n",
    "                    label.set_visible(True)\n",
    "                else:\n",
    "                    label.set_visible(False)\n",
    "\n",
    "            # задаем подписи для осей\n",
    "            ax.set(xlabel='time')        \n",
    "            ax.set(ylabel=metric)  \n",
    "\n",
    "            # задаем заголовок графика\n",
    "            ax.set_title(metric)\n",
    "\n",
    "            # укажим что нижняя граница оси y=0\n",
    "            ax.set(ylim=(0, None))\n",
    "\n",
    "            # отправляем график в телеграм\n",
    "            plot_object = io.BytesIO()\n",
    "            ax.figure.savefig(plot_object)\n",
    "            plot_object.seek(0)\n",
    "            plot_object.name='{0}.png'.format(metric)\n",
    "\n",
    "            #print(msg)\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            #отправляем сообщение и графики в телеграм, если выявлена аномалия\n",
    "            bot.sendMessage(chat_id=chat_id, text=msg)\n",
    "            bot.sendPhoto(chat_id=chat_id, photo=plot_object)    \n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    #вызов всех функций\n",
    "    df_cube_feed_7day = extract_feed_7day()\n",
    "    df_cube_feed_today = extract_feed_today()\n",
    "    df_cube_users_message = extract_users_message()\n",
    "    df_cube_sent_message = extract_sent_message()\n",
    "    df_cube_sent_message_today = extract_sent_message_today()\n",
    "\n",
    "\n",
    "    data_feed =  transform_data_feed(df_cube_feed_7day, df_cube_feed_today) \n",
    "\n",
    "    data_feed_dau = create_data_feed_dau(data_feed)\n",
    "    data_feed_views = create_data_feed_views(data_feed)\n",
    "    data_feed_likes = create_data_feed_likes(data_feed)\n",
    "    data_feed_ctr = create_data_feed_ctr(data_feed)\n",
    "    data_sent_message = transform_data_sent_message(df_cube_sent_message, df_cube_sent_message_today)\n",
    "\n",
    "    run_alerts(data_feed_dau, metrics = 'users_feed')\n",
    "    run_alerts(data_feed_views, metrics = 'views')\n",
    "    run_alerts(data_feed_likes, metrics = 'likes')\n",
    "    run_alerts(data_feed_ctr, metrics = 'ctr')\n",
    "    run_alerts(df_cube_users_message, metrics = 'users_message')\n",
    "    run_alerts(data_sent_message, metrics = 'sent_message')\n",
    "\n",
    "\n",
    "aksakal_my_alert = aksakal_my_alert() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
